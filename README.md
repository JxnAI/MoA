# MoA

 The Mixture-of-Agents (MoA) methodology represents a groundbreaking approach in artificial intelligence, designed to synergize the capabilities of multiple large language
 models. By operating through a layered architecture, MoA empowers each model agent to contribute effectively to the overall task, thereby surpassing the performance
 limitations of individual models.

 This innovative methodology involves integrating several LLMs across multiple layers, where each layer acts as a team of agents. Each agent processes information from the
 preceding layer, allowing for continuous refinement and enhancement of outputs. This layered structure enables the MoA to outperform established models like GPT-4 Omni,
 achieving scores of 65.1% on benchmarks such as AlpacaEval 2.0, compared to GPT-4 Omni's 57.5%, using solely open-source models.

## Examples

- business_idea.py
- test_manager.py (Agent UI)

Read the docs at <a href="https://docs.phidata.com" target="_blank" rel="noopener noreferrer">docs.phidata.com</a>
